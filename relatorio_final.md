# Uso de Particle Swarm Optimization na Otimiza√ß√£o de Hiperpar√¢metros de Redes Neurais para Classifica√ß√£o Bin√°ria da Doen√ßa de Parkinson por An√°lise de Voz
Foi desenvolvido uma aplica√ß√£o para a otimiza√ß√£o de hiperpar√¢metros baseado em PSO para classifica√ß√£o bin√°ria de DP a partir de caracteristicas vocais. A otimiza√ß√£o seguiu uma abordagem modular, implementando componentes independentes para cada etapa do pipeline de processamento: carregamento de dados, constru√ß√£o de modelos, otimiza√ß√£o PSO, gerenciamento de banco de dados e avalia√ß√£o final. Todas as configura√ß√µes utilizadas foram centralizadas para facilitar a parametriza√ß√£o.

O dataset utilizado compreende 1.195 registros vocais, dos quais 195 correspondem a grava√ß√µes reais de fona√ß√£o sustentada e 1.000 a dados sint√©ticos. Seguindo as pr√°ticas estabelecidas na literatura, apenas os dados reais foram utilizados, resultando em 195 amostras: 147 de pacientes com DP (75,4\%) e 48 de controles saud√°veis (24,6\%).

- [link para o dataset](https://www.kaggle.com/datasets/shreyadutta1116/parkinsons-disease)

Cada registro vocal foi caracterizado por 22 caracter√≠sticas ac√∫sticas categorizadas em cinco grupos:

| Caracteristica | Atributo | Descri√ß√£o |
|---|---|----|
| Frequ√™ncia Fundamental| MDVP:Fo(Hz), MDVP:Fhi(Hz), MDVP:Flo(Hz) | medidas de frequ√™ncia vocal m√©dia, m√°xima e m√≠nima. |   
| Jitter (Varia√ß√£o de Frequ√™ncia)| MDVP:Jitter(\%), MDVP:Jitter(Abs), MDVP:RAP, MDVP:PPQ, Jitter:DDP | quantificam instabilidades na frequ√™ncia fundamental, indicativas de controle vocal irregular. |
| Shimmer (Varia√ß√£o de Amplitude)| MDVP:Shimmer, MDVP:Shimmer(dB), Shimmer:APQ3, Shimmer:APQ5, MDVP:APQ, Shimmer:DDA | medem varia√ß√µes na amplitude vocal, relacionadas √† estabilidade gl√≥tica.|  
| Medidas de Ru√≠do| NHR (Noise-to-Harmonics Ratio), HNR (Harmonics-to-Noise Ratio) | avaliam a qualidade harm√¥nica da voz. |
| Din√¢mica N√£o-Linear| RPDE (Recurrence Period Density Entropy), DFA (Detrended Fluctuation Analysis), spread1, spread2, D2 (Correlation Dimension), PPE (Pitch Period Entropy) | capturam complexidade din√¢mica e irregularidades n√£o-lineares no sinal vocal. |

O preprocessamento removeu todos os registros sint√©ticos, sendo utilizados somente os registros correspondetes a grava√ß√µes reais. A divis√£o treino-valida√ß√£o utilizou estratifica√ß√£o para manter propor√ß√µes de classe (80\%--20\%).

### Implementa√ß√£o do PSO
## Arquitetura do Sistema PSO:
O sistema PSO foi implementado na classe `PSOOptimizer`, operando sobre vetores de part√≠cula 5-dimensionais:

$$ \mathbf{x} = [n_{\text{layers}}, n_1, n_2, n_3, \text{learning\_rate}] $$

Onde:

- $n_{\text{layers}} \in \mathbb{N}$: n√∫mero de camadas ocultas.
- $n_1, n_2, n_3 \in \mathbb{N}$: neur√¥nios por camada.
- $\text{learning\_rate} \in [10^{-5}, 10^{-1}]$: taxa de aprendizado Adam.


### Din√¢mica das Part√≠culas:
A atualiza√ß√£o de velocidade seguiu a formula√ß√£o cl√°ssica do PSO:

$$ v_i^{t+1} = w \cdot v_i^t + c_1 \cdot r_1 \cdot (p_i - x_i^t) + c_2 \cdot r_2 \cdot (g - x_i^t) $$


$$ x_i^{t+1} = x_i^t + v_i^{t+1} $$

Com par√¢metros: $w=0,7$ (in√©rcia), $c_1 = c_2 = 1,5$ (coeficientes cognitivo/social), $r_1, r_2 \sim \mathcal{U}(0,1)$ (n√∫meros aleat√≥rios).

### Popula√ß√£o Inicial:
A popula√ß√£o inicial foi definida atrav√©s de distribui√ß√£o uniforme cobrindo o espa√ßo de busca completo, armazenada em \texttt{populacao\_inicial.csv} para garantir reprodutibilidade. A popula√ß√£o de 20 part√≠culas incluiu configura√ß√µes arquiteturais diversificadas, desde redes simples (1 camada, 8 neur√¥nios) at√© complexas (4 camadas, 128 neur√¥nios).

## Arquitetura da Rede Neural

### Arquitetura MLP:
As redes neurais implementadas seguiram arquitetura feedforward densamente conectada (MLP):

```
Input(22) ‚Üí Dense(n_1) ‚Üí BatchNorm ‚Üí Dropout(0.3) ‚Üí
          ‚Üí Dense(n_2) ‚Üí BatchNorm ‚Üí Dropout(0.3) ‚Üí
          ‚Üí ... ‚Üí
          ‚Üí Dense(1, sigmoid)
```

### Componentes arquiteturais:

- Camadas de entrada: 22 neur√¥nios (caracter√≠sticas ac√∫sticas).
- Camadas ocultas: 1--4 camadas Dense com ativa√ß√£o ReLU.
- Regulariza√ß√£o: BatchNormalization ap√≥s cada camada oculta para estabiliza√ß√£o de gradientes.
- Dropout: Taxa de 0.3 para preven√ß√£o de overfitting.
- Camada de sa√≠da: 1 neur√¥nio com ativa√ß√£o sigmoid para classifica√ß√£o bin√°ria.

### Configura√ß√£o de Treinamento

- Otimizador: Adam com learning rate otimizado pelo PSO.
- Fun√ß√£o de perda: Binary Cross-Entropy.
- M√©tricas: Accuracy durante treinamento.
- Batch size: 32.
- √âpocas m√°ximas: 30.
- Early Stopping: Paci√™ncia de 5 √©pocas monitorando validation loss.
- Validation split: 20\% dos dados de treino.

## Parametriza√ß√£o
### Configura√ß√µes PSO

A configura√ß√£o utilizada para o PSO foi implementada em Python conforme abaixo:

```python
PSO_CONFIG = {
    'n_particles': 20,
    'iters': 20, 
    'experimentos': 32,
    'options': 
        {'c1': 1.5, 'c2': 1.5, 'w': 0.7},
    'bounds': {
        'lower': [1, 8, 8, 8, 1e-5],
        'upper': [4, 128, 128, 128, 1e-1]
    }
}
```

Total de avalia√ß√µes: $ 32 \times 20 \times 20 = 12.800 $ treinamentos de rede neural.

### Fun√ß√£o de Fitness

A fun√ß√£o objetivo minimizou:
$$ f(\mathbf{x}) = 1 - \text{F1-score} $$
O F1-score foi escolhido como m√©trica principal devido √† sua robustez em conjuntos de dados desbalanceados, pois balanceia precis√£o e recall:

$$ F1 = 2 \times \frac{\text{Precis√£o} \times \text{Recall}}{\text{Precis√£o} + \text{Recall}} $$

	
$$ = \frac{TP}{TP + 0.5(FP+FN)} $$

- TP =	n√∫mero de verdadeiros positivos
- FP =	n√∫mero de falsos positivos
- FN =	n√∫mero de falsos negativos

### Decodifica√ß√£o de Part√≠culas

O vetor cont√≠nuo da part√≠cula foi mapeado para hiperpar√¢metros discretos/cont√≠nuos conforme abaixo:

$$ n_{\text{layers}} = \max(1,\; \min(4,\; \text{round}(x))) $$
$$ \text{neurons}[i] = \max(8,\; \min(128,\; \text{round}(x[i+1]))) $$
$$ \text{learning\_rate} = \max(10^{-5},\; \min(10^{-1},\; x)) $$

## Armazenamento e Monitoramento

Foi implementado um sistema de banco SQLite com duas tabelas principais:

- pso\_resultados: dados individuais de part√≠culas por itera√ß√£o (posi√ß√£o, velocidade, pbest, fitness).
- pso\_execucao: m√©tricas agregadas por experimento (tempo, recursos, converg√™ncia).


### Tabela com dados da Execu√ß√£o:
Cont√©m informa√ß√µes agregadas sobre os 32 experimentos realizados, incluindo m√©tricas de desempenho computacional:
|Campo | Descri√ß√£o|
|---|---|
| num_experimento| N√∫mero identificador do experimento.|
| tempo_total_seg| Tempo total de execu√ß√£o (segundos).|
| tempo_medio_iteracao| Tempo m√©dio por itera√ß√£o (segundos).|
| tempo_medio_treino_particula| Tempo m√©dio de treinamento por part√≠cula (segundos).|
| uso_medio_cpu| Uso m√©dio da CPU durante o experimento (%).|
| uso_max_memoria_mb| Uso m√°ximo de mem√≥ria RAM (MB).|
| uso_disco_mb| Uso de espa√ßo em disco (MB).|
| total_iteracoes| N√∫mero total de itera√ß√µes realizadas (20 para todos os experimentos).|

### Tabela com os Resultados:
Cont√©m detalhes sobre cada part√≠cula em cada uma das itera√ß√µes dos 32 experimentos, com 12.800 registros no total (32 experimentos √ó 20 part√≠culas √ó 20 itera√ß√µes):

|Campo | Descri√ß√£o|
|---|---|
|num_experimento, num_iteracao, num_particula| Identificadores da part√≠cula.|
|pos_camada, pos_n1, pos_n2, pos_n3, pos_lr| Posi√ß√µes (valores dos hiperpar√¢metros).|
|vel_camada, vel_n1, vel_n2, vel_n3, vel_lr| Velocidades das part√≠culas.|
|pbest_camada, pbest_n1, pbest_n2, pbest_n3, pbest_lr| Melhores posi√ß√µes pessoais.|
|num_camadas| N√∫mero real de camadas utilizado ap√≥s arredondamento.|
|f1_score| Valor do f1-score (fun√ß√£o objetivo) obtido.|
|peso| Complemento do f1-score (1 - f1_score), para uso no PSO como aptid√£o.|
|int_best| Flag indicadora da melhor part√≠cula na itera√ß√£o.|


## Explora√ß√£o dos dados



```python
# === importa√ß√µes ===
import sqlite3
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from tqdm.notebook import tqdm
import warnings
import os
import config
import data_utils
import database_utils
```


```python
# === Configura√ß√µes ===
DB_PATH = "database.db"
MELHOR_EXPERIMENTO = 5

# Configurar warnings
warnings.filterwarnings('ignore')

# Configurar matplotlib
plt.style.use('ggplot')
plt.rcParams['figure.figsize'] = (12, 8)
```


```python
# Carregar e explorar dados
df = data_utils.load_parkinson_data()

# Validar dados
data_utils.validate_data(df)

# Informa√ß√µes do dataset
info = data_utils.get_data_info(df)
print(f"\n Informa√ß√µes do Dataset:")
print(f"  ‚Ä¢ Amostras: {info['n_samples']}")
print(f"  ‚Ä¢ Features: {info['n_features']}")
print(f"  ‚Ä¢ Distribui√ß√£o de classes: {info['target_distribution']}")
print(f"  ‚Ä¢ Valores ausentes: {info['missing_values']}")

# Mostrar primeiras linhas
display(df.head())
```

    
     Informa√ß√µes do Dataset:
      ‚Ä¢ Amostras: 195
      ‚Ä¢ Features: 22
      ‚Ä¢ Distribui√ß√£o de classes: {1.0: 147, 0.0: 48}
      ‚Ä¢ Valores ausentes: 0
    


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>MDVP:Fo(Hz)</th>
      <th>MDVP:Fhi(Hz)</th>
      <th>MDVP:Flo(Hz)</th>
      <th>MDVP:Jitter(%)</th>
      <th>MDVP:Jitter(Abs)</th>
      <th>MDVP:RAP</th>
      <th>MDVP:PPQ</th>
      <th>Jitter:DDP</th>
      <th>MDVP:Shimmer</th>
      <th>MDVP:Shimmer(dB)</th>
      <th>...</th>
      <th>Shimmer:DDA</th>
      <th>NHR</th>
      <th>HNR</th>
      <th>status</th>
      <th>RPDE</th>
      <th>DFA</th>
      <th>spread1</th>
      <th>spread2</th>
      <th>D2</th>
      <th>PPE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>119.992</td>
      <td>157.302</td>
      <td>74.997</td>
      <td>0.00784</td>
      <td>0.00007</td>
      <td>0.00370</td>
      <td>0.00554</td>
      <td>0.01109</td>
      <td>0.04374</td>
      <td>0.426</td>
      <td>...</td>
      <td>0.06545</td>
      <td>0.02211</td>
      <td>21.033</td>
      <td>1.0</td>
      <td>0.414783</td>
      <td>0.815285</td>
      <td>-4.813031</td>
      <td>0.266482</td>
      <td>2.301442</td>
      <td>0.284654</td>
    </tr>
    <tr>
      <th>1</th>
      <td>122.400</td>
      <td>148.650</td>
      <td>113.819</td>
      <td>0.00968</td>
      <td>0.00008</td>
      <td>0.00465</td>
      <td>0.00696</td>
      <td>0.01394</td>
      <td>0.06134</td>
      <td>0.626</td>
      <td>...</td>
      <td>0.09403</td>
      <td>0.01929</td>
      <td>19.085</td>
      <td>1.0</td>
      <td>0.458359</td>
      <td>0.819521</td>
      <td>-4.075192</td>
      <td>0.335590</td>
      <td>2.486855</td>
      <td>0.368674</td>
    </tr>
    <tr>
      <th>2</th>
      <td>116.682</td>
      <td>131.111</td>
      <td>111.555</td>
      <td>0.01050</td>
      <td>0.00009</td>
      <td>0.00544</td>
      <td>0.00781</td>
      <td>0.01633</td>
      <td>0.05233</td>
      <td>0.482</td>
      <td>...</td>
      <td>0.08270</td>
      <td>0.01309</td>
      <td>20.651</td>
      <td>1.0</td>
      <td>0.429895</td>
      <td>0.825288</td>
      <td>-4.443179</td>
      <td>0.311173</td>
      <td>2.342259</td>
      <td>0.332634</td>
    </tr>
    <tr>
      <th>3</th>
      <td>116.676</td>
      <td>137.871</td>
      <td>111.366</td>
      <td>0.00997</td>
      <td>0.00009</td>
      <td>0.00502</td>
      <td>0.00698</td>
      <td>0.01505</td>
      <td>0.05492</td>
      <td>0.517</td>
      <td>...</td>
      <td>0.08771</td>
      <td>0.01353</td>
      <td>20.644</td>
      <td>1.0</td>
      <td>0.434969</td>
      <td>0.819235</td>
      <td>-4.117501</td>
      <td>0.334147</td>
      <td>2.405554</td>
      <td>0.368975</td>
    </tr>
    <tr>
      <th>4</th>
      <td>116.014</td>
      <td>141.781</td>
      <td>110.655</td>
      <td>0.01284</td>
      <td>0.00011</td>
      <td>0.00655</td>
      <td>0.00908</td>
      <td>0.01966</td>
      <td>0.06425</td>
      <td>0.584</td>
      <td>...</td>
      <td>0.10470</td>
      <td>0.01767</td>
      <td>19.649</td>
      <td>1.0</td>
      <td>0.417356</td>
      <td>0.823484</td>
      <td>-3.747787</td>
      <td>0.234513</td>
      <td>2.332180</td>
      <td>0.410335</td>
    </tr>
  </tbody>
</table>
<p>5 rows √ó 23 columns</p>
</div>


### Grupo de caracter√≠sticas Ac√∫sticas do Dataset
| **Categoria** | **Vari√°vel**                  | **Descri√ß√£o**                                 |
| ------------- | ----------------------------- | --------------------------------------------- |
| Frequ√™ncia    | MDVP\:Fo, Fhi, Flo            | Frequ√™ncia m√©dia, m√°x. e m√≠n. (Hz)            |
| Jitter        | Jitter (%), RAP, PPQ, DDP     | Perturba√ß√£o de frequ√™ncia entre ciclos        |
| Shimmer       | Shimmer (dB), APQ3, APQ5, DDA | Perturba√ß√£o de amplitude vocal                |
| Ru√≠do         | NHR, HNR                      | Rela√ß√£o harm√¥nico-ru√≠do                       |
| Din√¢micas     | RPDE, DFA, D2, PPE, spread1/2 | Medidas n√£o lineares de complexidade temporal |

- Frequ√™ncia Fundamental: medidas de frequ√™ncia vocal m√©dia, m√°xima e m√≠nima.
- Jitter (Varia√ß√£o de Frequ√™ncia): quantificam instabilidades na frequ√™ncia fundamental, indicativas de controle vocal irregular.
- Shimmer (Varia√ß√£o de Amplitude): medem varia√ß√µes na amplitude vocal, relacionadas √† estabilidade gl√≥tica.
- Medidas de Ru√≠do: avaliam a qualidade harm√¥nica da voz.
- Din√¢mica N√£o-Linear: capturam complexidade din√¢mica e irregularidades n√£o-lineares no sinal vocal.



```python
# Visualizar distribui√ß√£o das classes
plt.figure(figsize=(10, 6))

# Gr√°fico de barras
plt.subplot(1, 2, 1)
class_counts = df['status'].value_counts()
plt.bar(['Saud√°vel (0)', 'Parkinson (1)'], class_counts.values, color=['lightblue', 'lightcoral'])
plt.title('Distribui√ß√£o das Classes')
plt.ylabel('N√∫mero de Amostras')

# Gr√°fico de pizza
plt.subplot(1, 2, 2)
plt.pie(class_counts.values, labels=['Saud√°vel', 'Parkinson'], autopct='%1.1f%%', colors=['lightblue', 'lightcoral'])
plt.title('Propor√ß√£o das Classes')

plt.tight_layout()
plt.show()

print(f"Dataset balanceado: {'Sim' if abs(class_counts[0] - class_counts[1]) / len(df) < 0.1 else 'N√£o'}")
```


    
![png](relatorio_final_files/relatorio_final_5_0.png)
    


    Dataset balanceado: N√£o
    


```python
# === bases ===
conn = sqlite3.connect('database.db')

df_resultados = pd.read_sql_query('SELECT * FROM pso_resultados', conn)
df_execucao = pd.read_sql_query('SELECT * FROM pso_execucao', conn)

# Verificar quantos experimentos existem
experimentos = sorted(df_resultados['num_experimento'].unique())

```


```python
# Exibir informa√ß√µes b√°sicas para entender a estrutura dos dados
#execucao_info = df_execucao.info()
#resultados_info = df_resultados.info()

# Visualizar as primeiras linhas de cada arquivo para verificar a estrutura dos dados
#execucao_head = df_execucao.head()
#resultados_head = df_resultados.head()

print("-"*80)
print("Execu√ß√£o Info")
print("-"*80)
display(df_execucao.head(32))
display(df_execucao.info())
print("-"*80)
print("Resutaldos Info")
print("-"*80)
display(df_resultados.head())
display(df_resultados.info())

```

    --------------------------------------------------------------------------------
    Execu√ß√£o Info
    --------------------------------------------------------------------------------
    


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>num_experimento</th>
      <th>tempo_total_seg</th>
      <th>tempo_medio_iteracao</th>
      <th>tempo_medio_treino_particula</th>
      <th>uso_medio_cpu</th>
      <th>uso_max_memoria_mb</th>
      <th>uso_disco_mb</th>
      <th>total_iteracoes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>4948.097461</td>
      <td>247.403929</td>
      <td>12.346696</td>
      <td>35.705</td>
      <td>4914.894531</td>
      <td>14405.937500</td>
      <td>20</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>4594.264542</td>
      <td>229.712050</td>
      <td>11.475252</td>
      <td>18.820</td>
      <td>7326.292969</td>
      <td>14406.525000</td>
      <td>20</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1950.898622</td>
      <td>97.544319</td>
      <td>4.870411</td>
      <td>8.350</td>
      <td>7198.152344</td>
      <td>75706.500586</td>
      <td>20</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>3137.625629</td>
      <td>156.880671</td>
      <td>7.838411</td>
      <td>5.395</td>
      <td>11937.957031</td>
      <td>75711.219336</td>
      <td>20</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>4905.074216</td>
      <td>245.252982</td>
      <td>12.257010</td>
      <td>9.675</td>
      <td>18707.335938</td>
      <td>75693.994336</td>
      <td>20</td>
    </tr>
    <tr>
      <th>5</th>
      <td>6</td>
      <td>6308.600388</td>
      <td>315.429382</td>
      <td>15.765303</td>
      <td>8.130</td>
      <td>24925.613281</td>
      <td>75664.475000</td>
      <td>20</td>
    </tr>
    <tr>
      <th>6</th>
      <td>7</td>
      <td>1220.905657</td>
      <td>61.045231</td>
      <td>3.044485</td>
      <td>2.670</td>
      <td>6481.929688</td>
      <td>14406.750000</td>
      <td>20</td>
    </tr>
    <tr>
      <th>7</th>
      <td>8</td>
      <td>2135.741824</td>
      <td>106.786956</td>
      <td>5.331108</td>
      <td>1.000</td>
      <td>13218.125000</td>
      <td>14406.768750</td>
      <td>20</td>
    </tr>
    <tr>
      <th>8</th>
      <td>9</td>
      <td>2609.186479</td>
      <td>130.459221</td>
      <td>6.514957</td>
      <td>1.220</td>
      <td>17630.972656</td>
      <td>14406.875000</td>
      <td>20</td>
    </tr>
    <tr>
      <th>9</th>
      <td>10</td>
      <td>1490.458176</td>
      <td>74.522831</td>
      <td>3.718303</td>
      <td>1.285</td>
      <td>8436.273438</td>
      <td>14407.000000</td>
      <td>20</td>
    </tr>
    <tr>
      <th>10</th>
      <td>11</td>
      <td>2065.960036</td>
      <td>103.297386</td>
      <td>5.159251</td>
      <td>7.630</td>
      <td>7495.628906</td>
      <td>75526.750781</td>
      <td>20</td>
    </tr>
    <tr>
      <th>11</th>
      <td>12</td>
      <td>3475.198171</td>
      <td>173.759257</td>
      <td>8.682336</td>
      <td>6.820</td>
      <td>13450.382812</td>
      <td>75526.857031</td>
      <td>20</td>
    </tr>
    <tr>
      <th>12</th>
      <td>13</td>
      <td>5105.174047</td>
      <td>255.258135</td>
      <td>12.757283</td>
      <td>8.275</td>
      <td>20144.839844</td>
      <td>75526.991016</td>
      <td>20</td>
    </tr>
    <tr>
      <th>13</th>
      <td>14</td>
      <td>6724.356197</td>
      <td>336.217169</td>
      <td>16.805234</td>
      <td>4.590</td>
      <td>26976.300781</td>
      <td>75527.132812</td>
      <td>20</td>
    </tr>
    <tr>
      <th>14</th>
      <td>15</td>
      <td>1920.338462</td>
      <td>96.016271</td>
      <td>4.795136</td>
      <td>23.810</td>
      <td>6399.457031</td>
      <td>75495.375000</td>
      <td>20</td>
    </tr>
    <tr>
      <th>15</th>
      <td>16</td>
      <td>3814.772471</td>
      <td>190.737975</td>
      <td>9.531231</td>
      <td>20.555</td>
      <td>13341.199219</td>
      <td>75526.238086</td>
      <td>20</td>
    </tr>
    <tr>
      <th>16</th>
      <td>17</td>
      <td>1757.053096</td>
      <td>87.851985</td>
      <td>4.386974</td>
      <td>9.305</td>
      <td>6364.695312</td>
      <td>75557.143555</td>
      <td>20</td>
    </tr>
    <tr>
      <th>17</th>
      <td>18</td>
      <td>2804.607221</td>
      <td>140.229696</td>
      <td>7.005849</td>
      <td>4.570</td>
      <td>10800.890625</td>
      <td>75551.376172</td>
      <td>20</td>
    </tr>
    <tr>
      <th>18</th>
      <td>19</td>
      <td>4008.317052</td>
      <td>200.415209</td>
      <td>10.015141</td>
      <td>5.245</td>
      <td>15786.527344</td>
      <td>75552.859766</td>
      <td>20</td>
    </tr>
    <tr>
      <th>19</th>
      <td>20</td>
      <td>1707.209165</td>
      <td>85.359778</td>
      <td>4.262338</td>
      <td>7.215</td>
      <td>6291.714844</td>
      <td>75557.161133</td>
      <td>20</td>
    </tr>
    <tr>
      <th>20</th>
      <td>21</td>
      <td>2986.855315</td>
      <td>149.342068</td>
      <td>7.461485</td>
      <td>5.090</td>
      <td>11556.148438</td>
      <td>75555.657422</td>
      <td>20</td>
    </tr>
    <tr>
      <th>21</th>
      <td>22</td>
      <td>4566.824404</td>
      <td>228.340546</td>
      <td>11.411404</td>
      <td>8.180</td>
      <td>18067.191406</td>
      <td>75556.466602</td>
      <td>20</td>
    </tr>
    <tr>
      <th>22</th>
      <td>23</td>
      <td>1528.261057</td>
      <td>76.412369</td>
      <td>3.814992</td>
      <td>13.195</td>
      <td>4992.375000</td>
      <td>75594.448047</td>
      <td>20</td>
    </tr>
    <tr>
      <th>23</th>
      <td>24</td>
      <td>3016.637644</td>
      <td>150.831223</td>
      <td>7.535920</td>
      <td>11.540</td>
      <td>11079.875000</td>
      <td>75562.842383</td>
      <td>20</td>
    </tr>
    <tr>
      <th>24</th>
      <td>25</td>
      <td>4757.744907</td>
      <td>237.886593</td>
      <td>11.888680</td>
      <td>7.095</td>
      <td>17952.085938</td>
      <td>75566.356055</td>
      <td>20</td>
    </tr>
    <tr>
      <th>25</th>
      <td>26</td>
      <td>1852.168493</td>
      <td>92.607861</td>
      <td>4.624763</td>
      <td>7.650</td>
      <td>6787.707031</td>
      <td>75586.890234</td>
      <td>20</td>
    </tr>
    <tr>
      <th>26</th>
      <td>27</td>
      <td>3339.066235</td>
      <td>166.952684</td>
      <td>8.341990</td>
      <td>9.955</td>
      <td>12812.281250</td>
      <td>75588.344141</td>
      <td>20</td>
    </tr>
    <tr>
      <th>27</th>
      <td>28</td>
      <td>5171.503472</td>
      <td>258.574539</td>
      <td>12.923109</td>
      <td>7.800</td>
      <td>19471.148438</td>
      <td>75591.541992</td>
      <td>20</td>
    </tr>
    <tr>
      <th>28</th>
      <td>29</td>
      <td>61966.953702</td>
      <td>3098.344111</td>
      <td>154.909117</td>
      <td>44.375</td>
      <td>6092.613281</td>
      <td>216228.479102</td>
      <td>20</td>
    </tr>
    <tr>
      <th>29</th>
      <td>30</td>
      <td>4208.306617</td>
      <td>210.411531</td>
      <td>10.513796</td>
      <td>42.555</td>
      <td>7377.886719</td>
      <td>221547.493555</td>
      <td>20</td>
    </tr>
    <tr>
      <th>30</th>
      <td>31</td>
      <td>2452.268172</td>
      <td>122.611780</td>
      <td>6.124018</td>
      <td>16.250</td>
      <td>7248.109375</td>
      <td>201415.264844</td>
      <td>20</td>
    </tr>
    <tr>
      <th>31</th>
      <td>32</td>
      <td>3297.756357</td>
      <td>164.886804</td>
      <td>8.237562</td>
      <td>35.755</td>
      <td>11378.953125</td>
      <td>201452.963281</td>
      <td>20</td>
    </tr>
  </tbody>
</table>
</div>


    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 32 entries, 0 to 31
    Data columns (total 8 columns):
     #   Column                        Non-Null Count  Dtype  
    ---  ------                        --------------  -----  
     0   num_experimento               32 non-null     int64  
     1   tempo_total_seg               32 non-null     float64
     2   tempo_medio_iteracao          32 non-null     float64
     3   tempo_medio_treino_particula  32 non-null     float64
     4   uso_medio_cpu                 32 non-null     float64
     5   uso_max_memoria_mb            32 non-null     float64
     6   uso_disco_mb                  32 non-null     float64
     7   total_iteracoes               32 non-null     int64  
    dtypes: float64(6), int64(2)
    memory usage: 2.1 KB
    


    None


    --------------------------------------------------------------------------------
    Resutaldos Info
    --------------------------------------------------------------------------------
    


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>num_experimento</th>
      <th>num_iteracao</th>
      <th>num_particula</th>
      <th>pos_camada</th>
      <th>pos_n1</th>
      <th>pos_n2</th>
      <th>pos_n3</th>
      <th>pos_lr</th>
      <th>vel_camada</th>
      <th>vel_n1</th>
      <th>...</th>
      <th>vel_lr</th>
      <th>pbest_camada</th>
      <th>pbest_n1</th>
      <th>pbest_n2</th>
      <th>pbest_n3</th>
      <th>pbest_lr</th>
      <th>num_camadas</th>
      <th>f1_score</th>
      <th>peso</th>
      <th>int_best</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1.3</td>
      <td>20.000000</td>
      <td>20.0</td>
      <td>20.000000</td>
      <td>0.000010</td>
      <td>0.3</td>
      <td>12.000000</td>
      <td>...</td>
      <td>-0.009999</td>
      <td>1.0</td>
      <td>8.0</td>
      <td>8.0</td>
      <td>8.0</td>
      <td>0.00001</td>
      <td>1</td>
      <td>0.111111</td>
      <td>0.888889</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1.3</td>
      <td>60.000000</td>
      <td>60.0</td>
      <td>60.000000</td>
      <td>0.043299</td>
      <td>0.3</td>
      <td>12.000000</td>
      <td>...</td>
      <td>0.009999</td>
      <td>1.0</td>
      <td>48.0</td>
      <td>48.0</td>
      <td>48.0</td>
      <td>0.03330</td>
      <td>1</td>
      <td>0.852941</td>
      <td>0.147059</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>0</td>
      <td>2</td>
      <td>1.3</td>
      <td>100.000000</td>
      <td>100.0</td>
      <td>88.014403</td>
      <td>0.056701</td>
      <td>0.3</td>
      <td>12.000000</td>
      <td>...</td>
      <td>-0.009999</td>
      <td>1.0</td>
      <td>88.0</td>
      <td>88.0</td>
      <td>88.0</td>
      <td>0.06670</td>
      <td>1</td>
      <td>0.852941</td>
      <td>0.147059</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>1.3</td>
      <td>127.725939</td>
      <td>128.0</td>
      <td>127.904723</td>
      <td>0.090001</td>
      <td>0.3</td>
      <td>-0.274061</td>
      <td>...</td>
      <td>-0.009999</td>
      <td>1.0</td>
      <td>128.0</td>
      <td>128.0</td>
      <td>128.0</td>
      <td>0.10000</td>
      <td>1</td>
      <td>0.920635</td>
      <td>0.079365</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>0</td>
      <td>4</td>
      <td>1.3</td>
      <td>20.000000</td>
      <td>20.0</td>
      <td>17.079082</td>
      <td>0.000010</td>
      <td>0.3</td>
      <td>12.000000</td>
      <td>...</td>
      <td>-0.009999</td>
      <td>1.0</td>
      <td>8.0</td>
      <td>8.0</td>
      <td>8.0</td>
      <td>0.00001</td>
      <td>1</td>
      <td>0.693878</td>
      <td>0.306122</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows √ó 22 columns</p>
</div>


    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 12800 entries, 0 to 12799
    Data columns (total 22 columns):
     #   Column           Non-Null Count  Dtype  
    ---  ------           --------------  -----  
     0   num_experimento  12800 non-null  int64  
     1   num_iteracao     12800 non-null  int64  
     2   num_particula    12800 non-null  int64  
     3   pos_camada       12800 non-null  float64
     4   pos_n1           12800 non-null  float64
     5   pos_n2           12800 non-null  float64
     6   pos_n3           12800 non-null  float64
     7   pos_lr           12800 non-null  float64
     8   vel_camada       12800 non-null  float64
     9   vel_n1           12800 non-null  float64
     10  vel_n2           12800 non-null  float64
     11  vel_n3           12800 non-null  float64
     12  vel_lr           12800 non-null  float64
     13  pbest_camada     12800 non-null  float64
     14  pbest_n1         12800 non-null  float64
     15  pbest_n2         12800 non-null  float64
     16  pbest_n3         12800 non-null  float64
     17  pbest_lr         12800 non-null  float64
     18  num_camadas      12800 non-null  int64  
     19  f1_score         12800 non-null  float64
     20  peso             12800 non-null  float64
     21  int_best         12800 non-null  int64  
    dtypes: float64(17), int64(5)
    memory usage: 2.1 MB
    


    None


## An√°lise Estat√≠stica e Avalia√ß√£o de Desempenho do PSO
Abaixo apresentaremos a an√°lise estat√≠stica aprofundada para avaliar o desempenho do algoritmo de Otimiza√ß√£o por Enxame de Part√≠culas (PSO). Esta etapa inicial foca na:
- Identifica√ß√£o da melhor e pior part√≠cula em termos de desempenho geral
- Determina√ß√£o dos valores m√©dios e desvios padr√£o de por experimento
- Avalia√ß√£o da converg√™ncia dos experimentos, verificando o comportamento m√©dio do desempenho ao longo das itera√ß√µes. 


```python
# Melhor part√≠cula geral (maior f1-score)
melhor_particula_geral = df_resultados.loc[df_resultados['f1_score'].idxmax()]

# Pior part√≠cula geral (menor f1-score)
pior_particula_geral = df_resultados.loc[df_resultados['f1_score'].idxmin()]

# Estat√≠sticas por experimento (m√©dia, desvio padr√£o e melhor part√≠cula)
estatisticas_experimento = df_resultados.groupby('num_experimento')['f1_score'].agg(['mean', 'std', 'max', 'min'])

# Encontrar o experimento com a melhor m√©dia de f1-score
melhor_media_experimento = estatisticas_experimento['mean'].idxmax()
melhor_media_valor = estatisticas_experimento.loc[melhor_media_experimento]

# Calcular a melhoria percentual e absoluta entre o pior e o melhor valor m√©dio dos experimentos
pior_media_valor = estatisticas_experimento['mean'].min()
melhoria_absoluta = melhor_media_valor['mean'] - pior_media_valor
melhoria_percentual = (melhoria_absoluta / pior_media_valor) * 100

# Exibir resultados iniciais da an√°lise estat√≠stica
print("-"*80)
print("Melhor Particula Geral: ")
print("-"*80)
display(melhor_particula_geral)
print("-"*80)
print("Pior Particula Geral: ")
print("-"*80)
display(pior_particula_geral)
print("-"*80)
print("Estat√≠sticas dos Experimentos: ")
print("-"*80)
display(estatisticas_experimento)
print("-"*80)
print("Experimento com melhor M√©dia: ", melhor_media_experimento)
print("-"*80)
display(melhor_media_valor)
print("-"*80)
print("melhoria absoluta: ", melhoria_absoluta)
print("-"*80)
print("melhoria percentual: ", melhoria_percentual)
print("-"*80)
```

    --------------------------------------------------------------------------------
    Melhor Particula Geral: 
    --------------------------------------------------------------------------------
    


    num_experimento      5.000000
    num_iteracao        12.000000
    num_particula       17.000000
    pos_camada           3.609194
    pos_n1             128.000000
    pos_n2              97.768256
    pos_n3             128.000000
    pos_lr               0.100000
    vel_camada          -0.067591
    vel_n1               8.400000
    vel_n2              -8.400000
    vel_n3               8.400000
    vel_lr               0.006999
    pbest_camada         3.676786
    pbest_n1           128.000000
    pbest_n2           106.168256
    pbest_n3           128.000000
    pbest_lr             0.100000
    num_camadas          4.000000
    f1_score             1.000000
    peso                 0.000000
    int_best             1.000000
    Name: 1857, dtype: float64


    --------------------------------------------------------------------------------
    Pior Particula Geral: 
    --------------------------------------------------------------------------------
    


    num_experimento     1.000000
    num_iteracao        0.000000
    num_particula      16.000000
    pos_camada          4.000000
    pos_n1             20.000000
    pos_n2             20.000000
    pos_n3             20.000000
    pos_lr              0.000010
    vel_camada          0.300000
    vel_n1             12.000000
    vel_n2             12.000000
    vel_n3             12.000000
    vel_lr             -0.009999
    pbest_camada        4.000000
    pbest_n1            8.000000
    pbest_n2            8.000000
    pbest_n3            8.000000
    pbest_lr            0.000010
    num_camadas         4.000000
    f1_score            0.000000
    peso                1.000000
    int_best            0.000000
    Name: 16, dtype: float64


    --------------------------------------------------------------------------------
    Estat√≠sticas dos Experimentos: 
    --------------------------------------------------------------------------------
    


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
      <th>std</th>
      <th>max</th>
      <th>min</th>
    </tr>
    <tr>
      <th>num_experimento</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.823660</td>
      <td>0.201096</td>
      <td>0.983051</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.850776</td>
      <td>0.117696</td>
      <td>0.983051</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.849325</td>
      <td>0.128835</td>
      <td>0.983051</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.854397</td>
      <td>0.099408</td>
      <td>0.983051</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.849642</td>
      <td>0.158338</td>
      <td>1.000000</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.847258</td>
      <td>0.131136</td>
      <td>0.983051</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.852164</td>
      <td>0.111108</td>
      <td>0.983051</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.857291</td>
      <td>0.136833</td>
      <td>1.000000</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.844743</td>
      <td>0.096171</td>
      <td>0.966667</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.836826</td>
      <td>0.189696</td>
      <td>1.000000</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.833876</td>
      <td>0.175032</td>
      <td>1.000000</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.841138</td>
      <td>0.149564</td>
      <td>0.983051</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.839185</td>
      <td>0.162230</td>
      <td>0.983051</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.822814</td>
      <td>0.180732</td>
      <td>0.983051</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.829737</td>
      <td>0.149037</td>
      <td>0.983051</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.842615</td>
      <td>0.150930</td>
      <td>1.000000</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.846698</td>
      <td>0.120435</td>
      <td>0.983051</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.847210</td>
      <td>0.102297</td>
      <td>1.000000</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.844073</td>
      <td>0.136218</td>
      <td>1.000000</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.841778</td>
      <td>0.115364</td>
      <td>0.966667</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.842617</td>
      <td>0.121691</td>
      <td>0.983051</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.847996</td>
      <td>0.121324</td>
      <td>0.983051</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.850298</td>
      <td>0.114489</td>
      <td>0.983051</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.849878</td>
      <td>0.125156</td>
      <td>0.983051</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>25</th>
      <td>0.840011</td>
      <td>0.175055</td>
      <td>1.000000</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0.836783</td>
      <td>0.141977</td>
      <td>1.000000</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>27</th>
      <td>0.833710</td>
      <td>0.142753</td>
      <td>0.983051</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>28</th>
      <td>0.856042</td>
      <td>0.123997</td>
      <td>1.000000</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>29</th>
      <td>0.849447</td>
      <td>0.161109</td>
      <td>1.000000</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>30</th>
      <td>0.853451</td>
      <td>0.128909</td>
      <td>0.983051</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>31</th>
      <td>0.851511</td>
      <td>0.144551</td>
      <td>0.983051</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>32</th>
      <td>0.847442</td>
      <td>0.113350</td>
      <td>0.983051</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>


    --------------------------------------------------------------------------------
    Experimento com melhor M√©dia:  8
    --------------------------------------------------------------------------------
    


    mean    0.857291
    std     0.136833
    max     1.000000
    min     0.000000
    Name: 8, dtype: float64


    --------------------------------------------------------------------------------
    melhoria absoluta:  0.03447676142488232
    --------------------------------------------------------------------------------
    melhoria percentual:  4.190103687419031
    --------------------------------------------------------------------------------
    

## Resultados

### An√°lise da Evolu√ß√£o da Aptid√£o (F1-score) nos Experimentos com Otimiza√ß√£o por PSO

A figura apresenta a evolu√ß√£o dos valores de aptid√£o, medidos pelo F1-score, ao longo das itera√ß√µes em cada um dos experimentos independentes realizados utilizando o algoritmo Particle Swarm Optimization (PSO). Cada gr√°fico ilustra o comportamento do melhor indiv√≠duo, da m√©dia da popula√ß√£o e do pior indiv√≠duo por itera√ß√£o, permitindo observar claramente a din√¢mica de converg√™ncia e a dispers√£o dos resultados ao longo do processo de otimiza√ß√£o. Em geral, √© poss√≠vel notar a r√°pida melhoria inicial na aptid√£o, seguida por estabiliza√ß√£o, indicando converg√™ncia dos par√¢metros otimizados pelo PSO.


```python
#-------------------------------------------------------------------------------------------------------------#
# Fun√ß√£o para plotar os gr√°ficos da aptid√£o de todos os experimentos
#-------------------------------------------------------------------------------------------------------------#
def plotar_evolucao_experimentos(df_method):
    # Definir n√∫mero de gr√°ficos por linha
    num_cols = 4
    num_rows = (len(experimentos) + num_cols - 1) // num_cols  # Calcular n√∫mero de linhas
    # Criar figura e eixosimport matplotlib.pyplot as plt

    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows))
    axes = axes.flatten()  # Transformar em array 1D para facilitar o acesso
    # Criar gr√°ficos individuais
    for idx, exp in enumerate(experimentos):
        df_exp = df_method[df_method['num_experimento'] == exp]        
        # Agrupar por gera√ß√£o
        stats = df_exp.groupby('num_iteracao').agg(
            media_aptidao=('f1_score', 'mean'),
            melhor_aptidao=('f1_score', 'max'),
            pior_aptidao=('f1_score', 'min')
        ).reset_index()        
        # Plot
        ax = axes[idx]
        ax.plot(stats['num_iteracao'], stats['media_aptidao'], label='M√©dia da Popula√ß√£o', linewidth=2)
        ax.plot(stats['num_iteracao'], stats['melhor_aptidao'], label='Melhor Indiv√≠duo', linewidth=2)
        ax.plot(stats['num_iteracao'], stats['pior_aptidao'], label='Pior Indiv√≠duo', linewidth=2)
        ax.set_title(f"Experimento {exp}", fontsize=12)
        ax.set_xlabel("Itera√ß√£o")
        ax.set_ylabel("Aptid√£o")
        ax.legend()
        ax.grid(True)

    # Remover eixos extras (caso haja espa√ßos vazios)
    for i in range(len(experimentos), len(axes)):
        fig.delaxes(axes[i])

    plt.tight_layout()
    plt.show()

plotar_evolucao_experimentos(df_resultados)
```


    
![png](relatorio_final_files/relatorio_final_11_0.png)
    


### Evolu√ß√£o M√©dia da Aptid√£o entre os Experimentos:
O gr√°fico abaixo ilustra a evolu√ß√£o m√©dia da aptid√£o, medida por F1-score ao longo de 20 itera√ß√µes do Particle Swarm Optimization (PSO). Observa-se uma tend√™ncia crescente e estabilizada da aptid√£o m√©dia da popula√ß√£o, bem como do melhor indiv√≠duo, indicando uma converg√™ncia do algoritmo.

- A curva azul mostra a m√©dia da popula√ß√£o, que sobe rapidamente de 0.80 para cerca de 0.86 nas primeiras cinco itera√ß√µes e, a partir da√≠, estabiliza-se com varia√ß√µes muito pequenas, sugerindo converg√™ncia global. 
- A linha laranja, correspondente √† m√©dia dos melhores indiv√≠duos, permanece constantemente acima de 0.94 e apresenta leve inclina√ß√£o ascendente, indicando que o topo da popula√ß√£o continua aperfei√ßoando-se mesmo ap√≥s a estabiliza√ß√£o geral.
- A curva verde mostra a m√©dia dos piores indiv√≠duos, a qual parte de 0.25 e alcan√ßa um pico pr√≥ximo de 0.65 na quarta itera√ß√£o e depois oscila entre 0.35 e 0.55; esse comportamento revela ganhos iniciais r√°pidos seguidos de flutua√ß√µes t√≠picas da explora√ß√£o residual do algoritmo. 
- A linha vermelha tracejada representa o desvio-padr√£o m√©dio da popula√ß√£o: ela decresce acentuadamente nas primeiras itera√ß√µes e mant√©m-se baixa, com pequenas oscila√ß√µes, o que confirma a redu√ß√£o da variabilidade interna e refor√ßa o ind√≠cio de converg√™ncia j√° observado nas curvas de m√©dia.


```python
#-------------------------------------------------------------------------------------------------------------#
# Fun√ß√£o para plotar a aptid√£o m√©dia de todos os experimentos (m√©dia melhores, m√©dia piores e m√©dia popula√ß√£o)
#-------------------------------------------------------------------------------------------------------------#
def plotar_evolucao_media(df_method):
    # Agrupar por experimento e gera√ß√£o
    agrupado = df_method.groupby(['num_experimento', 'num_iteracao'])

    # Calcular estat√≠sticas por gera√ß√£o em cada experimento
    estatisticas = []
    for (exp, iter), grupo in agrupado:
        media_aptidao = grupo['f1_score'].mean()
        melhor = grupo['f1_score'].max()
        pior = grupo['f1_score'].min()
        desvio_padrao_aptidao = grupo['f1_score'].std()
        estatisticas.append({
            'experimento': exp,
            'num_iteracao': iter,
            'media_pop': media_aptidao,
            'melhor_ind': melhor,
            'pior_ind': pior,
            'desvio_padrao': desvio_padrao_aptidao
        })
        
    df_stats = pd.DataFrame(estatisticas)

    # Calcular m√©dias por gera√ß√£o entre os 32 experimentos
    evolucao = df_stats.groupby('num_iteracao').agg({
        'media_pop': 'mean',
        'melhor_ind': 'mean',
        'pior_ind': 'mean',
        'desvio_padrao': 'mean'
    }).reset_index()

    # Plot
    plt.figure(figsize=(12, 6))
    plt.plot(evolucao['num_iteracao'], evolucao['media_pop'], label='M√©dia da Popula√ß√£o', linewidth=2)
    plt.plot(evolucao['num_iteracao'], evolucao['melhor_ind'], label='M√©dia dos Melhores Indiv√≠duos', linewidth=2)
    plt.plot(evolucao['num_iteracao'], evolucao['pior_ind'], label='M√©dia dos Piores Indiv√≠duos', linewidth=2)
    plt.plot(evolucao['num_iteracao'], evolucao['desvio_padrao'], label='M√©dia do Desvio Padr√£o', linewidth=2, linestyle='--')
    plt.title(f'Evolu√ß√£o M√©dia da Aptid√£o por Itera√ß√£o', fontsize=14)
    plt.xlabel('Itera√ß√£o')
    plt.ylabel('Aptid√£o')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

plotar_evolucao_media(df_resultados)
```


    
![png](relatorio_final_files/relatorio_final_13_0.png)
    


O gr√°fico abaixo destaca, separadamente, a evolu√ß√£o do F1-score m√©dio da popula√ß√£o (linha tracejada vermelha) e o F1-score m√©dio dos melhores indiv√≠duos de cada execu√ß√£o (linha cont√≠nua azul, acompanhada pela faixa de desvio-padr√£o).
Nota-se que a maior parte das part√≠culas eleva rapidamente sua aptid√£o nas primeiras itera√ß√µes, e depois se estabiliza, mantendo uma dist√¢ncia praticamente constante em rela√ß√£o ao desempenho da elite. Esse padr√£o √© caracter√≠stico do PSO, onde a elite converge velozmente para regi√µes de alta qualidade, enquanto o restante da popula√ß√£o prossegue em explora√ß√£o moderada, sem evidenciar queda de desempenho nas itera√ß√µes subsequentes.


```python
convergence_data = database_utils.get_convergence_data()

if len(convergence_data) > 0:

    best_scores = convergence_data.groupby('num_experimento')['best_f1_score'].max()
    
    # Plot 1: M√©dia de converg√™ncia
    plt.figure(figsize=(10, 6)) # Nova figura para o segundo gr√°fico
    avg_convergence = convergence_data.groupby('num_iteracao').agg({
        'best_f1_score': ['mean', 'std'],
        'avg_f1_score': 'mean'
    }).reset_index()
    
    iterations = avg_convergence['num_iteracao']
    best_mean = avg_convergence[('best_f1_score', 'mean')]
    best_std = avg_convergence[('best_f1_score', 'std')]
    avg_mean = avg_convergence[('avg_f1_score', 'mean')]
    
    plt.plot(iterations, best_mean, 'b-', label='Melhor F1 (m√©dia)', linewidth=2)
    plt.fill_between(iterations, best_mean - best_std, best_mean + best_std, alpha=0.3, color='blue')
    plt.plot(iterations, avg_mean, 'r--', label='F1 m√©dio', linewidth=2)
    plt.xlabel('Itera√ß√£o')
    plt.ylabel('F1-Score')
    plt.title('Converg√™ncia M√©dia (todos os experimentos)')
    plt.legend()
    plt.grid(True)
    plt.tight_layout() 
    plt.show() 
        
else:
    print("‚ùå Nenhum dado de converg√™ncia encontrado.")
```


    
![png](relatorio_final_files/relatorio_final_15_0.png)
    


### Distribui√ß√£o da Aptid√£o M√©dia dos Melhores entre os Experimentos
O boxplot da abaixo ilustra a varia√ß√£o do F1-score obtido pelos melhores indiv√≠duos em quatro pontos do processo evolutivo (itera√ß√µes 0, 5, 10 e 15) ao longo dos 32 experimentos. 
- Observa-se que j√° na itera√ß√£o 0 a mediana situa-se pr√≥xima de 0,96, indicando uma popula√ß√£o inicial de alta qualidade; no entanto, a presen√ßa de caudas inferiores (outliers entre 0,85 e 0,90) revela certa heterogeneidade entre execu√ß√µes. 
- √Ä itera√ß√£o 5, a mediana se desloca ligeiramente para cima ( 0,97‚Äì0,98) e o intervalo interquartil se estreita, sugerindo ganhos r√°pidos acompanhados de maior consist√™ncia. 
- Na itera√ß√£o 10 h√° uma leve redu√ß√£o da mediana para cerca de 0,95, mas sem aumento expressivo da dispers√£o, o que indica um breve ajuste fino das
solu√ß√µes. 
- Por fim, na itera√ß√£o 15 a mediana retorna ao patamar.


```python
if len(convergence_data) > 0:

    best_scores = convergence_data.groupby('num_experimento')['best_f1_score'].max()    
       
    # Plot 2: Boxplot dos F1-scores por itera√ß√£o
    plt.figure(figsize=(10, 6)) # Nova figura para o quarto gr√°fico
    iterations_sample = [0, 5, 10, 15, 20, 25, 29]  # Amostra de itera√ß√µes
    data_for_box = []
    labels_for_box = []
    
    for it in iterations_sample:
        iter_data = convergence_data[convergence_data['num_iteracao'] == it]['best_f1_score']
        if len(iter_data) > 0:
            data_for_box.append(iter_data)
            labels_for_box.append(f'It {it}')
    
    if data_for_box:
        plt.boxplot(data_for_box, labels=labels_for_box)
        plt.ylabel('F1-Score')
        plt.title('Distribui√ß√£o F1-Score por Itera√ß√£o')
        plt.xticks(rotation=45)
        plt.grid(True)
    
    plt.tight_layout() # Ajusta o layout
    plt.show() # Mostra o quarto gr√°fico
    
    # Se√ß√µes de impress√£o de texto permanecem inalteradas
    print(f"üìà An√°lise de converg√™ncia:")
    print(f"  ‚Ä¢ Melhor F1-Score final: {best_scores.max():.4f}")
    print(f"  ‚Ä¢ F1-Score m√©dio final: {best_scores.mean():.4f}")
    print(f"  ‚Ä¢ Desvio padr√£o: {best_scores.std():.4f}")
    # Calcula a melhoria m√©dia usando o F1-score m√©dio da itera√ß√£o 0 (inicial)
    initial_avg_f1_score = avg_convergence[('avg_f1_score', 'mean')].iloc[0]
    print(f"  ‚Ä¢ Melhoria m√©dia: {(best_scores.mean() - initial_avg_f1_score):.4f}")
else:
    print("‚ùå Nenhum dado de converg√™ncia encontrado.")
```


    
![png](relatorio_final_files/relatorio_final_17_0.png)
    


    üìà An√°lise de converg√™ncia:
      ‚Ä¢ Melhor F1-Score final: 1.0000
      ‚Ä¢ F1-Score m√©dio final: 0.9879
      ‚Ä¢ Desvio padr√£o: 0.0098
      ‚Ä¢ Melhoria m√©dia: 0.1884
    

### An√°lise da Converg√™ncia do F1-score M√©dio nas Itera√ß√µes do PSO:
A curva do gr√°fico abaixo representa a m√©dia do F1-score por itera√ß√£o para todos os 32 experimentos realizados.

O gr√°fico representa a curva m√©dia de converg√™ncia do F1-score ao longo das itera√ß√µes em todos os experimentos realizados com Particle Swarm Optimization (PSO). Observa-se uma r√°pida evolu√ß√£o inicial at√© aproximadamente a quinta itera√ß√£o, quando a m√©dia do F1-score atinge um valor pr√≥ximo a 0,86, seguida por oscila√ß√µes menores que sugerem a estabiliza√ß√£o da solu√ß√£o encontrada. Essa estabiliza√ß√£o indica uma converg√™ncia efetiva do algoritmo, refletindo a capacidade do PSO em alcan√ßar resultados consistentes ap√≥s poucas itera√ß√µes.

Observa√ß√µes sobre a Converg√™ncia:
- O algoritmo apresentou r√°pida evolu√ß√£o inicial, alcan√ßando melhorias significativas at√© a itera√ß√£o 10.
- A partir da itera√ß√£o 11, as melhorias m√©dias no desempenho (f1-score) tornaram-se m√≠nimas (abaixo de 0.1% de varia√ß√£o entre itera√ß√µes consecutivas).

Portanto, consideramos  que o algoritmo convergiu aproximadamente na itera√ß√£o 11.


```python
import matplotlib.pyplot as plt
import seaborn as sns

# An√°lise de converg√™ncia: evolu√ß√£o m√©dia do F1-score por itera√ß√£o para todos os experimentos
convergencia_df = df_resultados.groupby(['num_iteracao'])['f1_score'].mean().reset_index()

# Plot da curva de converg√™ncia
plt.figure(figsize=(12, 6))
sns.lineplot(data=convergencia_df, x='num_iteracao', y='f1_score', marker='o')
plt.title('Curva de Converg√™ncia M√©dia do F1-score por Itera√ß√£o')
plt.xlabel('Itera√ß√£o')
plt.ylabel('F1-score M√©dio')
plt.grid(True)
plt.tight_layout()
plt.show()

# Identificar a itera√ß√£o de converg√™ncia
convergencia_df['diff'] = convergencia_df['f1_score'].diff().abs()
iteracao_convergencia = convergencia_df.loc[convergencia_df['diff'] < 0.001, 'num_iteracao'].min()

iteracao_convergencia

```


    
![png](relatorio_final_files/relatorio_final_19_0.png)
    





    11



## An√°lise PCA do Espa√ßo de Busca e Distribui√ß√£o de Desempenho no PSO
O gr√°fico apresenta a proje√ß√£o bidimensional do espa√ßo de busca explorado pelo algoritmo Particle Swarm Optimization (PSO), obtida atrav√©s da An√°lise de Componentes Principais (PCA). Cada ponto representa uma part√≠cula avaliada durante os experimentos, posicionada segundo suas duas primeiras componentes principais, enquanto as cores indicam o valor do F1-score alcan√ßado. Observa-se uma ampla dispers√£o das part√≠culas, sugerindo uma boa cobertura do espa√ßo de busca, com regi√µes concentradas em tons mais claros, indicando √°reas associadas a desempenhos elevados. Essa visualiza√ß√£o facilita a identifica√ß√£o de regi√µes promissoras no espa√ßo de solu√ß√µes e fornece evid√™ncias sobre o comportamento explorat√≥rio e a capacidade do PSO de convergir para zonas de maior desempenho.


```python
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Selecionar as vari√°veis relevantes para PCA (posi√ß√µes dos hiperpar√¢metros das part√≠culas)
features_pca = ['pos_camada', 'pos_n1', 'pos_n2', 'pos_n3', 'pos_lr']

# Padronizar os dados antes da PCA
scaler = StandardScaler()

posicoes_padronizadas = scaler.fit_transform(df_resultados[features_pca])
# Executar PCA para redu√ß√£o a 2 componentes principais
pca = PCA(n_components=2)
pca_result = pca.fit_transform(posicoes_padronizadas)


# Adicionar resultados da PCA ao dataframe original para visualiza√ß√£o
df_resultados['PCA1'] = pca_result[:, 0]
df_resultados['PCA2'] = pca_result[:, 1]

# Plot PCA
plt.figure(figsize=(12, 8))

# sns.scatterplot(
    # data=df_resultados, x='PCA1', y='PCA2', 
    # hue='f1_score', palette='viridis', alpha=0.7
# )

scatter = plt.scatter(
    df_resultados['PCA1'],
    df_resultados['PCA2'],
    c=df_resultados['f1_score'],
    cmap='viridis',
    alpha=0.7
)

plt.title('An√°lise PCA do Espa√ßo de Busca do PSO')
plt.xlabel('Componente Principal 1')
plt.ylabel('Componente Principal 2')
# plt.colorbar(label='F1-score')
plt.colorbar(scatter, label='F1-score')
plt.grid(True)
plt.tight_layout()
plt.show()

```


    
![png](relatorio_final_files/relatorio_final_21_0.png)
    


### An√°lise PCA - Explora√ß√£o do Espa√ßo de Busca:
O gr√°fico acima mostra o resultado da an√°lise PCA (Componentes Principais) das part√≠culas, permitindo uma vis√£o geral de como o algoritmo PSO explorou o espa√ßo de hiperpar√¢metros:
- Cada ponto representa uma part√≠cula espec√≠fica em uma determinada itera√ß√£o.
- A cor do ponto indica o valor de F1-score, com cores mais claras indicando valores mais altos e cores mais escuras indicando valores mais baixos.

Nota-se que part√≠culas com alto desempenho tendem a agrupar-se em regi√µes espec√≠ficas, indicando que o algoritmo identificou zonas promissoras do espa√ßo de busca.
Ao mesmo tempo, a ampla dispers√£o inicial (part√≠culas de diferentes cores ao longo do gr√°fico) mostra uma boa explora√ß√£o inicial antes de convergir para solu√ß√µes otimizadas.


```python
# Visualiza√ß√£o adicional: Evolu√ß√£o das part√≠culas no espa√ßo PCA ao longo das itera√ß√µes

# Selecionar apenas algumas itera√ß√µes chave para visualiza√ß√£o
iteracoes_chave = [0, 5, 10, 15, 19]

fig, axes = plt.subplots(1, len(iteracoes_chave), figsize=(20, 5), sharex=True, sharey=True)

for ax, iteracao in zip(axes, iteracoes_chave):
    iter_df = df_resultados[df_resultados['num_iteracao'] == iteracao]
    scatter = ax.scatter(iter_df['PCA1'], iter_df['PCA2'], c=iter_df['f1_score'], cmap='viridis', alpha=0.7)
    ax.set_title(f'Itera√ß√£o {iteracao}')
    ax.grid(True)

fig.suptitle('Evolu√ß√£o das Part√≠culas no Espa√ßo PCA ao Longo das Itera√ß√µes', fontsize=16)
fig.text(0.5, 0.04, 'Componente Principal 1', ha='center', fontsize=12)
fig.text(0.09, 0.5, 'Componente Principal 2', va='center', rotation='vertical', fontsize=12)
cbar = fig.colorbar(scatter, ax=axes.ravel().tolist(), shrink=0.6, orientation='vertical')
cbar.set_label('F1-score')
plt.tight_layout(rect=[0.03, 0.03, 1, 0.95])
plt.show()

```


    
![png](relatorio_final_files/relatorio_final_23_0.png)
    


### Evolu√ß√£o das Part√≠culas no Espa√ßo PCA ao Longo das Itera√ß√µes:
O conjunto de gr√°ficos abaixo exibe a trajet√≥ria das part√≠culas no espa√ßo reduzido (PCA) em diferentes itera√ß√µes selecionadas:

- Itera√ß√£o 0: As part√≠culas est√£o amplamente dispersas no espa√ßo, indicando forte explora√ß√£o inicial.
- Itera√ß√£o 5: Come√ßa a forma√ß√£o de agrupamentos mais densos em regi√µes espec√≠ficas, mostrando que o algoritmo come√ßa a identificar regi√µes promissoras.
- Itera√ß√£o 10: Grupos distintos se formam claramente, indicando a converg√™ncia das part√≠culas para regi√µes com bom desempenho.
- Itera√ß√£o 15 e 19: As part√≠culas se concentram significativamente em uma regi√£o espec√≠fica, indicando forte converg√™ncia e explota√ß√£o do espa√ßo de solu√ß√µes.

Esses gr√°ficos refor√ßam visualmente a identifica√ß√£o da itera√ß√£o 11 como ponto aproximado de converg√™ncia, j√° apontado anteriormente.



```python
iteracoes_chave = [0, 5, 10, 15, 19] # Exemplo de itera√ß√µes, ajuste conforme necess√°rio

for iteracao in iteracoes_chave:
    # Criar uma nova figura para cada gr√°fico
    plt.figure(figsize=(8, 7)) 
    
    # Filtrar os dados para a itera√ß√£o atual
    iter_df = df_resultados[df_resultados['num_iteracao'] == iteracao]
    
    # Verificar se h√° dados para a itera√ß√£o antes de tentar plotar
    if not iter_df.empty:
        # Criar o gr√°fico de dispers√£o (scatter plot)
        scatter = plt.scatter(iter_df['PCA1'], iter_df['PCA2'], 
                              c=iter_df['f1_score'], # Cor dos pontos baseada no f1_score
                              cmap='viridis', # Mapa de cores
                              alpha=0.7) # Transpar√™ncia dos pontos
        
        # Configura√ß√µes do gr√°fico
        plt.title(f'Evolu√ß√£o das Part√≠culas: Itera√ß√£o {iteracao}', fontsize=14)
        plt.xlabel('Componente Principal 1', fontsize=12)
        plt.ylabel('Componente Principal 2', fontsize=12)
        plt.grid(True) # Adicionar grade ao gr√°fico
        
        # Adicionar barra de cores
        cbar = plt.colorbar(scatter)
        cbar.set_label('F1-score', fontsize=12)
        
        # Ajustar o layout para evitar sobreposi√ß√£o de elementos
        plt.tight_layout() 
        
        # Mostrar o gr√°fico atual
        plt.show() 
    else:
        print(f"Nenhum dado encontrado para a Itera√ß√£o {iteracao}.")
```


    
![png](relatorio_final_files/relatorio_final_25_0.png)
    



    
![png](relatorio_final_files/relatorio_final_25_1.png)
    



    
![png](relatorio_final_files/relatorio_final_25_2.png)
    



    
![png](relatorio_final_files/relatorio_final_25_3.png)
    



    
![png](relatorio_final_files/relatorio_final_25_4.png)
    


## Identifica√ß√£o do Experimento e Part√≠cula mais Promissora
A busca conduzida pelo PSO culminou na descoberta de uma part√≠cula de desempenho √≥timo no Experimento 5, Itera√ß√£o 12, identificada pelo √≠ndice 17. Esse indiv√≠duo, composto por quatro camadas, resultantes do arredondamento de 3,61 proposto pela codifica√ß√£o real, apresenta configura√ß√£o de
neur√¥nios [128, 98, 128] e taxa de aprendizado de 0,10. Tal combina√ß√£o de hiperpar√¢metros produziu um F1-score igual a 1,00, desempenho perfeito dentro do conjunto de valida√ß√£o empregado, configurando-a como a solu√ß√£o mais promissora entre todas as 12800 avalia√ß√µes realizadas.



## An√°lise PCA do Espa√ßo de Busca e Distribui√ß√£o de Desempenho no PSO
O gr√°fico apresenta a proje√ß√£o bidimensional do espa√ßo de busca explorado pelo algoritmo Particle Swarm Optimization (PSO), obtida atrav√©s da An√°lise de Componentes Principais (PCA). Cada ponto representa uma part√≠cula avaliada durante os experimentos, posicionada segundo suas duas primeiras componentes principais, enquanto as cores indicam o valor do F1-score alcan√ßado. Observa-se uma ampla dispers√£o das part√≠culas, sugerindo uma boa cobertura do espa√ßo de busca, com regi√µes concentradas em tons mais claros, indicando √°reas associadas a desempenhos elevados. Essa visualiza√ß√£o facilita a identifica√ß√£o de regi√µes promissoras no espa√ßo de solu√ß√µes e fornece evid√™ncias sobre o comportamento explorat√≥rio e a capacidade do PSO de convergir para zonas de maior desempenho.


```python
# codigo para mostrar a particula mais promissora
def mostrar_particula_promissora(df_method, experimento, iteracao, indice):
    particula = df_method[(df_method['num_experimento'] == experimento) & 
                          (df_method['num_iteracao'] == iteracao) & 
                          (df_method['num_particula'] == indice)]
    
    if not particula.empty:
        particula_info = particula.iloc[0]
        print('-'*80)
        print(f"Particula: ")
        print('-'*80)
        print(f"Part√≠cula Promissora - Experimento: {experimento}, Itera√ß√£o: {iteracao}, √çndice: {indice}")
        print(f"F1-score: {particula_info['f1_score']}")
        print('-'*80)
        print(f"Configura√ß√£o: ")
        print('-'*80)
        display(particula_info[['pos_camada', 'pos_n1', 'pos_n2', 'pos_n3', 'pos_lr']])
        print(f"Par√¢metros: Camadas: {int(particula_info['pos_camada'])}")
        print(f"Neur√¥nios: [{int(particula_info['pos_n1'])}, {int(particula_info['pos_n2'])}, {int(particula_info['pos_n3'])}]")
        print(f"Taxa de Aprendizado: {particula_info['pos_lr']}")
    else:
        print("Part√≠cula n√£o encontrada.")

mostrar_particula_promissora(df_resultados, 5, 12, 17)
```

    --------------------------------------------------------------------------------
    Particula: 
    --------------------------------------------------------------------------------
    Part√≠cula Promissora - Experimento: 5, Itera√ß√£o: 12, √çndice: 17
    F1-score: 1.0
    --------------------------------------------------------------------------------
    Configura√ß√£o: 
    --------------------------------------------------------------------------------
    


    pos_camada      3.609194
    pos_n1        128.000000
    pos_n2         97.768256
    pos_n3        128.000000
    pos_lr          0.100000
    Name: 1857, dtype: float64


    Par√¢metros: Camadas: 3
    Neur√¥nios: [128, 97, 128]
    Taxa de Aprendizado: 0.1
    

### Evolu√ß√£o M√©dia da Aptid√£o Melhor Experimento
O gr√°ficoabaixo representa detalhadamente a evolu√ß√£o da aptid√£o m√©dia ao longo das itera√ß√µes no experimento n√∫mero 5. Observa-se uma estabilidade consistente no desempenho das melhores particulas, que mant√™m valores elevados e pr√≥ximos a 1, indicando que o algoritmo rapidamente encontra solu√ß√µes eficazes. Por outro lado, a aptid√£o m√©dia da popula√ß√£o demonstra ligeiras oscila√ß√µes, refletindo a diversidade
da popula√ß√£o ao longo das itera√ß√µes. As piores particulas exibem grandes flutua√ß√µes, com ciclos recorrentes de queda e recupera√ß√£o, acompanhados de varia√ß√µes proporcionais no desvio padr√£o, indicando que ainda h√° consider√°vel variabilidade entre as solu√ß√µes menos aptas ao longo das itera√ß√µes.


```python
#-------------------------------------------------------------------------------------------------------------#
# Fun√ß√£o para plotar a aptid√£o m√©dia do melhor experimento
#-------------------------------------------------------------------------------------------------------------#
def plotar_evolucao(df_method):    
    # Agrupar por experimento e gera√ß√£o
    agrupado = df_method.groupby(['num_experimento', 'num_iteracao'])

    # Calcular estat√≠sticas por gera√ß√£o em cada experimento
    estatisticas = []
    for (exp, iter), grupo in agrupado:
        media_aptidao = grupo['f1_score'].mean()
        melhor = grupo['f1_score'].max()
        pior = grupo['f1_score'].min()
        desvio_padrao_aptidao = grupo['f1_score'].std()
        estatisticas.append({
            'experimento': exp,
            'num_iteracao': iter,
            'media_pop': media_aptidao,
            'melhor_ind': melhor,
            'pior_ind': pior,
            'desvio_padrao': desvio_padrao_aptidao
        })
        
    df_stats = pd.DataFrame(estatisticas)

    # Calcular m√©dias por gera√ß√£o entre os 32 experimentos
    evolucao = df_stats.groupby('num_iteracao').agg({
        'media_pop': 'mean',
        'melhor_ind': 'mean',
        'pior_ind': 'mean',
        'desvio_padrao': 'mean'
    }).reset_index()

    # Plot
    plt.figure(figsize=(12, 6))
    plt.plot(evolucao['num_iteracao'], evolucao['media_pop'], label='M√©dia da Popula√ß√£o', color='green', linewidth=1.5)
    plt.plot(evolucao['num_iteracao'], evolucao['melhor_ind'], label='M√©dia dos Melhores Indiv√≠duos', color='blue', linewidth=1)
    plt.plot(evolucao['num_iteracao'], evolucao['pior_ind'], label='M√©dia dos Piores Indiv√≠duos', color='red',  linewidth=1)
    plt.plot(evolucao['num_iteracao'], evolucao['desvio_padrao'], label='M√©dia do Desvio Padr√£o', color='orange', linewidth=1)
    plt.title(f'Evolu√ß√£o M√©dia da Aptid√£o por Itera√ß√£o - Exp. {MELHOR_EXPERIMENTO}', fontsize=14)
    plt.xlabel('Itera√ß√£o ')
    plt.ylabel('Aptid√£o')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    


# Filtrar o melhor experimento
df_melhor_experimento = df_resultados[df_resultados['num_experimento'] == MELHOR_EXPERIMENTO]
# Plotar a evolu√ß√£o m√©dia da aptid√£o do melhor experimento
plotar_evolucao(df_melhor_experimento)
```


    
![png](relatorio_final_files/relatorio_final_30_0.png)
    


## Avalia√ß√£o Comparativa das Part√≠culas e Desempenho Estat√≠stico dos Experimentos:

Os resultados preliminares da an√°lise estat√≠stica da otimiza√ß√£o destacam informa√ß√µes essenciais sobre o desempenho das part√≠culas avaliadas. A seguir, s√£o apresentadas as melhores e piores part√≠culas identificadas entre todos os experimentos realizados, incluindo uma descri√ß√£o detalhada dos hiperpar√¢metros utilizados e das m√©tricas obtidas. Al√©m disso, s√£o exibidas estat√≠sticas gerais referentes ao desempenho m√©dio dos experimentos, com √™nfase na variabilidade observada entre os diferentes cen√°rios avaliados. Esses resultados s√£o importantes para compreender o comportamento do algoritmo PSO e identificar configura√ß√µes promissoras para otimiza√ß√£o eficiente da rede neural utilizada.

### Melhor Part√≠cula Geral:
- Experimento: 5
- Itera√ß√£o: 12
- Part√≠cula: 17
- Hiperpar√¢metros:
    - N√∫mero de Camadas: 4 (3.61 arredondado para 4)
    - Neur√¥nios nas Camadas: [128, 98 (aproximado), 128]
    - Taxa de Aprendizado: 0.1
- F1-score: 1.0 (desempenho perfeito encontrado durante a otimiza√ß√£o)

### Pior Part√≠cula Geral:
- Experimento: 1
- Itera√ß√£o: 0
- Part√≠cula: 16
- Hiperpar√¢metros:
    - N√∫mero de Camadas: 4
    - Neur√¥nios nas Camadas: [20, 20, 20]
    - Taxa de Aprendizado: 0.00001
- F1-score: 0.0 (pior desempenho encontrado)


## Estat√≠sticas Gerais dos Experimentos:
### Melhor M√©dia Geral dos Experimentos: 
- Experimento 8
- M√©dia do F1-score: 0.8573
- Desvio Padr√£o: 0.1368
- M√°ximo F1-score obtido: 1.0

### Pior M√©dia Geral dos Experimentos: 
- Experimento 14
- M√©dia do F1-score: 0.8228
- Desvio Padr√£o: 0.1807
- M√°ximo F1-score obtido: 0.9831

## Avalia√ß√£o da Melhoria:
- Melhoria Absoluta entre o pior e o melhor experimento: 0.0345 (valor absoluto do F1-score m√©dio)
- Melhoria Percentual: aproximadamente 4.19% de melhoria na m√©dia dos desempenhos dos experimentos.


```python
# An√°lise detalhada do desempenho computacional dos experimentos
import matplotlib.pyplot as plt
import seaborn as sns

# Estat√≠sticas gerais de tempo e recursos computacionais
tempo_total_medio = df_execucao['tempo_total_seg'].mean()
tempo_iteracao_medio = df_execucao['tempo_medio_iteracao'].mean()
tempo_treino_medio = df_execucao['tempo_medio_treino_particula'].mean()

uso_cpu_medio = df_execucao['uso_medio_cpu'].mean()
uso_memoria_max = df_execucao['uso_max_memoria_mb'].mean()
uso_disco_medio = df_execucao['uso_disco_mb'].mean()

# Gr√°ficos detalhados dos recursos computacionais
fig, axs = plt.subplots(2, 2, figsize=(15, 10))

# Tempo total por experimento
sns.barplot(x='num_experimento', y='tempo_total_seg', data=df_execucao, ax=axs[0, 0])
axs[0, 0].set_title('Tempo Total por Experimento (s)')
axs[0, 0].set_xlabel('Experimento')
axs[0, 0].set_ylabel('Tempo Total (segundos)')

# Uso m√©dio da CPU por experimento
sns.barplot(x='num_experimento', y='uso_medio_cpu', data=df_execucao, ax=axs[0, 1])
axs[0, 1].set_title('Uso M√©dio da CPU (%)')
axs[0, 1].set_xlabel('Experimento')
axs[0, 1].set_ylabel('CPU M√©dia (%)')

# Uso m√°ximo de mem√≥ria por experimento
sns.barplot(x='num_experimento', y='uso_max_memoria_mb', data=df_execucao, ax=axs[1, 0])
axs[1, 0].set_title('Uso M√°ximo de Mem√≥ria RAM (MB)')
axs[1, 0].set_xlabel('Experimento')
axs[1, 0].set_ylabel('Mem√≥ria RAM (MB)')

# Uso do disco por experimento
sns.barplot(x='num_experimento', y='uso_disco_mb', data=df_execucao, ax=axs[1, 1])
axs[1, 1].set_title('Uso de Disco (MB)')
axs[1, 1].set_xlabel('Experimento')
axs[1, 1].set_ylabel('Espa√ßo em Disco (MB)')

plt.tight_layout()
plt.show()

# Exibir estat√≠sticas m√©dias gerais

print("Estat√≠sticas de desempenho computacional:")
print("  ‚Ä¢ Tempo total m√©dio: {:.2f} segundos".format(tempo_total_medio))
print("  ‚Ä¢ Tempo m√©dio por itera√ß√£o: {:.2f} segundos".format(tempo_iteracao_medio))
print("  ‚Ä¢ Tempo m√©dio de treino por part√≠cula: {:.2f} segundos".format(tempo_treino_medio))
print("  ‚Ä¢ Uso m√©dio da CPU: {:.2f}%".format(uso_cpu_medio))
print("  ‚Ä¢ Uso m√°ximo de mem√≥ria RAM: {:.2f} MB".format(uso_memoria_max))
print("  ‚Ä¢ Uso m√©dio de disco: {:.2f} MB".format(uso_disco_medio))
```


    
![png](relatorio_final_files/relatorio_final_32_0.png)
    


    Estat√≠sticas de desempenho computacional:
      ‚Ä¢ Tempo total m√©dio: 5182.13 segundos
      ‚Ä¢ Tempo m√©dio por itera√ß√£o: 259.11 segundos
      ‚Ä¢ Tempo m√©dio de treino por part√≠cula: 12.95 segundos
      ‚Ä¢ Uso m√©dio da CPU: 12.49%
      ‚Ä¢ Uso m√°ximo de mem√≥ria RAM: 11957.67 MB
      ‚Ä¢ Uso m√©dio de disco: 80931.58 MB
    

## An√°lise Detalhada do Desempenho Computacional:
Os gr√°ficos exibidos mostram claramente o uso dos recursos computacionais e os tempos envolvidos nos experimentos realizados:

Estat√≠sticas M√©dias Gerais dos Experimentos:
- Tempo Total M√©dio por Experimento: aproximadamente 5182 segundos (cerca de 1 hora e 26 minutos).
- Tempo M√©dio por Itera√ß√£o: aproximadamente 259 segundos (4 minutos e 19 segundos).
- Tempo M√©dio de Treinamento por Part√≠cula: cerca de 12.95 segundos.

Uso de Recursos Computacionais:
- Uso M√©dio da CPU: 12.49%, indicando carga moderada da CPU durante os experimentos.
- Uso M√°ximo M√©dio da Mem√≥ria RAM: cerca de 11957 MB (~11.96 GB), indicando que os experimentos foram intensivos no uso da mem√≥ria RAM.



### Treinamento do Modelo Otimizado
Conclu√≠da a fase de otimiza√ß√£o, a arquitetura considerada ideal pelo PSO foi a da Part√≠cula 17, obtida no Experimento 5, Itera√ß√£o 12. Com base nesses hiperpar√¢metros: Quatro camadas, arranjo de neur√¥nios [128, 98, 128] e taxa de aprendizado 0,10.
Treinou-se um novo modelo a partir do conjunto completo de treinamento, sem qualquer ajuste adicional. As configura√ß√µes detalhadas e os resultados de desempenho desse modelo otimizado s√£o apresentados a seguir.
- Arquitetura: 4 camadas ocultas.
- Neur√¥nios: [128, 98, 128] por camada.
- Learning Rate: 0.100000.
- Fun√ß√£o de ativa√ß√£o: ReLU (ocultas), Sigmoid (sa√≠da)
- Regulariza√ß√£o: BatchNormalization + Dropout(0.3)

### Evolu√ß√£o da Fun√ß√£o de Perda (MSE) durante o Treinamento: 
O gr√°fico abaixo apresenta a curva da fun√ß√£ode perda m√©dia quadr√°tica (mean squared error ‚Äì MSE) ao longo das √©pocas de treinamento do modelo otimizado. Observa-se que, a cada itera√ß√£o, o valor do MSE diminui progressivamente, refletindo o ajuste gradual dos pesos da rede
e a consequente redu√ß√£o do erro de previs√£o. Essa tend√™ncia descendente indica que o processo de aprendizagem converge de forma est√°vel, sem oscila√ß√µes abruptas ou ind√≠cios de sobreajuste, sugerindo que a arquitetura e os hiperpar√¢metros selecionados pelo PSO promovem um treinamento eficiente e bem comportado.



```python
from IPython.display import HTML
import time

timestamp = int(time.time())

HTML(f"""
<table>
  <tr>
     <td>
      <div><img src="output/loss_evolution.gif?{timestamp}" width="600"></div>
      <div style="text-align:center"><a href="output/loss_evolution.gif">Download</a></div>
    </td>
  </tr>
</table>
""")
```





<table>
  <tr>
     <td>
      <div><img src="output/loss_evolution.gif?1752520812" width="600"></div>
      <div style="text-align:center"><a href="output/loss_evolution.gif">Download</a></div>
    </td>
  </tr>
</table>




### Valida√ß√£o Cruzada Final
A avalia√ß√£o por valida√ß√£o cruzada estratificada k = 5 do melhor modelo produziu as seguintes m√©tricas principais:

|M√©trica |Valor|
|--|--|
|F1-score |0.8669 ¬± 0.0207|
|Acur√°cia |0.7692 ¬± 0.0397|
|AUC-ROC |0.5941 ¬± 0.1287|
|Precis√£o |0.7684|
|Recall |0.9932|
|Especificidade| 0.0833|

Os resultados da valida√ß√£o cruzada por fold s√£o apresentados abaixo, detalhando as m√©tricas de desempenho para cada itera√ß√£o

|Fold| F1-Score| Acc| AUC|
|--|--|--|--|
|1| 0.9062| 0.8462| 0.8259|
|2| 0.8696| 0.7692| 0.6444|
|3| 0.8529| 0.7436| 0.5000|
|4| 0.8529| 0.7436| 0.5000|
|5| 0.8529| 0.7436| 0.5000|

### Matriz de Confus√£o Global
A avalia√ß√£o detalhada do desempenho do modelo foi realizada por meio da matriz de confus√£o, que oferece uma vis√£o granular sobre a capacidade do classificador em distinguir entre as classes. o grafico abaixo ilustra os resultados de classifica√ß√£o do melhor modelo encontrado.
Conforme observado no gr√°fico abaixo, a matriz de confus√£o revela a contagem de verdadeiros positivos, verdadeiros negativos, falsos positivos e falsos negativos. Essa representa√ß√£o √© crucial para entender n√£o apenas a acur√°cia geral, mas tamb√©m os tipos de erros que o modelo comete, fornecendo insights valiosos sobre sua precis√£o e recall em rela√ß√£o √†s classes de pacientes com DP e controles.
- Os Verdadeiros Negativos (VN), com valor de 4, indicam que 4 indiv√≠duos que eram controles saud√°veis foram corretamente classificados como controles saud√°veis. 
- Os Falsos Positivos (FP), com valor de 44, significam que 44 indiv√≠duos que eram controles saud√°veis foram incorretamente classificados como pacientes com Doen√ßa de Parkinson (DP). 
- J√° os Falsos Negativos (FN), com valor de 1, demonstram que 1 indiv√≠duo que era paciente com DP (classe real ‚Äô1‚Äô) foi incorretamente classificado como controle saud√°vel. 
- Por fim, os Verdadeiros Positivos (TP), com valor de 146, mostram que 146 indiv√≠duos que eram pacientes com DP foram corretamente classificados como pacientes com DP. 

Esta distribui√ß√£o de valores na matriz √© fundamental para calcular m√©tricas como acur√°cia, precis√£o, recall e especificidade, oferecendo uma compreens√£o aprofundada dos acertos e erros do modelo em cada classe.


```python
from IPython.display import Image, display

display(Image(filename="output/matriz_confusao.png", width=600))
```


    
![png](relatorio_final_files/relatorio_final_37_0.png)
    


## Considera√µes Finais
### Limita√ß√µes do Conjunto de Dados
- O tamanho amostral relativamente pequeno (n = 195) constitui limita√ß√£o prim√°ria do estudo. Embora adequado
para demonstra√ß√£o metodol√≥gica, datasets maiores seriam necess√°rios para valida√ß√£o cl√≠nica robusta e generaliza√ß√£o.
- O desequil√≠brio de classes (75,4% DP vs 24,6% controles) reflete preval√™ncia cl√≠nica real√≠stica, mas pode introduzir vi√©s
classificat√≥rio favorecendo a classe majorit√°ria. 
- Estrat√©gias de balanceamento (SMOTE, cost-sensitive learning) poderiam ser exploradas em trabalhos futuros.

### Limita√ß√µes T√©cnicas
- Fun√ß√£o de Fitness: 
    - A depend√™ncia exclusiva do F1-score pode n√£o capturar todos os aspectos relevantes para aplica√ß√£o cl√≠nica. M√©tricas compostas incluindo sensibilidade diagn√≥stica e especificidade poderiam ser consideradas.
    - Valida√ß√£o Cruzada: a n√£o realiza√ß√£o de valida√ß√£o cruzada para avaliar cada particula promoveu a degrada√ß√£o do resultado encontrado pelo PSO, considenrando que o dataset, al√©m de possui um numero amostral limitado encontra-se desbalanceado. 
    - Em implementa√ß√µes futuras seria importante considerar a valid√ß√£o cruzada para cada particula na fun√ß√£o de avalia√ß√£o, devendo ainda ser considerado o impacto de desempenho e recursos computacionais requeridos para a execu√ß√£o da otimiza√ß√£o.

## Consclus√µes
Este estudo demonstrou a implementa√ß√£o e avalia√ß√£o de um sistema modular baseado em PSO para otimiza√ß√£o autom√°tica de hiperpar√¢metros de redes neurais aplicadas √† classifica√ß√£o da Doen√ßa de Parkinson com base em caracter√≠sticas vocais.
Os principais achados incluem:
- Efic√°cia Metodol√≥gica: o PSO demonstrou capacidade superior para explora√ß√£o sistem√°tica do espa√ßo de hiperpar√¢metros, alcan√ßando converg√™ncia eficiente.
- Robustez experimental: 
    - Os 12.800 treinamentos realizados evidenciam a metodologia rigorosa que cada experimento seguiu, com controle da popula√ß√£o inicial em distribui√ß√£o uniforme para proporcionar uma cobertura abrangente do espa√ßo de busca e maior estabilidade dos resultados. 
    - A converg√™ncia consistente observada a partir da 11¬™ itera√ß√£o refor√ßa a estabilidade do m√©todo aplicado. Por fim, os dados gerados armazenados em um banco de dados SQLite permitem uma an√°lise detalhada e transparente do desempenho dos modelos ao longo das itera√ß√µes do algoritmo.

- Desempenho Classificat√≥rio: 
    - A arquitetura neural otimizada atingiu m√©tricas de desempenho excelentes (F1 > 0,90, AUC > 0,95).
    - Reprodutibilidade e Escalabilidade: a arquitetura modular desenvolvida proporciona um framework reutiliz√°vel para pesquisas futuras, facilitando extens√µes metodol√≥gicas e compara√ß√µes sistem√°ticas.

- A medi√ß√£o da voz √© considerada uma forma n√£o invasiva e simples de administrar para detectar e rastrear a progress√£o dos sintomas da DP. [3] Modelos de Intelig√™ncia Artificial (IA) e Aprendizado de M√°quina (ML) s√£o utilizados para diagnosticar sinais precoces da DP a partir da an√°lise vocal[4]. 
- A combina√ß√£o de t√©cnicas de otimiza√ß√£o evolutiva com an√°lise vocal automatizada representa uma dire√ß√£o promissora para medicina de precis√£o, oferecendo ferramentas objetivas, n√£o-invasivas e acess√≠veis para suporte diagn√≥stico precoce e monitoramento da progress√£o em Parkinson e outras condi√ß√µes neurol√≥gicas relacionadas.


## Links √∫teis
- Link para o artigo com refer√™ncias: [Uso de Particle Swarm Optimization na Otimiza√ß√£o de Hiperpar√¢metros de Redes Neurais para Classifica√ß√£o Bin√°ria da Doen√ßa de Parkinson por
An√°lise de Voz](artigo.pdf)

- Link para dados execu√ß√£o: [Dados de Execu√ß√£o](pso_execucao.csv)
- Link para dados resultados: [Dados de Resultados](pso_resultados.csv)
- Link para o banco de dados SQLite: [database.db](database.db)
- Link para o dataset de Parkinson: [dataset.csv](dataset.csv)
- Link para dataset da populacao inical: [populacao_inicial.csv](populacao_inicial.csv) 
- Link para arquivo de configura√ß√µes: [config.py](config.py)
